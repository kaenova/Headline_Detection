{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Notebooks\n",
    "\n",
    "Beberapa preprocessing untuk membuat skenario sebagai penelitian\n",
    "\n",
    "Preprocessing sesuai dengan IndoBERTweet\n",
    "1. Sama seperti preprosesing unutk membuat IndoBERTweet (lowercasing, change user, change emoji, change http url) (scenario 1)\n",
    "2. Nomor 1 dan melakukan stemming (scenario 2)\n",
    "\n",
    "Preprocessing untuk only full text tanpa adanya Twitter atribute  \n",
    "1. Lowecasing, Menghilangkan atribut url dan mention dalam text, change emoji  (scenario 3)\n",
    "2. Nomor 3 dan melakukan stemming (scenario 4)\n",
    "\n",
    "Preprocessing untuk only full text tanpa adanya Twitter attribute dan emoji agar hanya full text yang mirip dengan headline di berita-berita\n",
    "1. Lowecasing, Menghilangkan atribut url dan mention dalam text, menghilangkan emoji. (scenario 5)\n",
    "2. Nomor 5 dan melakukan stemming (scenario 6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate some processing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NDETCStemmer import NDETCStemmer, CustomModelDownloader\n",
    "from kaelib.processor.NDETCStemmerWraper import NDETCStemmerWraper\n",
    "\n",
    "downloader = CustomModelDownloader(\n",
    "    model_1=\"https://is3.cloudhost.id/s3.kaenova.my.id/NDETCStemmer/Model/w2vec_wiki_id_case\",\n",
    "    model_2=\"https://is3.cloudhost.id/s3.kaenova.my.id/NDETCStemmer/Model/w2vec_wiki_id_case.trainables.syn1neg.npy\",\n",
    "    model_3=\"https://is3.cloudhost.id/s3.kaenova.my.id/NDETCStemmer/Model/w2vec_wiki_id_case.wv.vectors.npy\"\n",
    ")\n",
    "\n",
    "original_stemmer=NDETCStemmer(custom_downloader=downloader)\n",
    "stemmer = NDETCStemmerWraper(original_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n",
      "Scenario 1: :astonished_face::astonished_face: miliarder rusia oleg tinkov pada senin (31/10/2022), mengaku telah  melepaskan kewarganegaraan rusianya karena konflik di ukraina. HTTPURL @USER\n",
      "Scenario 2: :astonished_face::astonished_face: miliarder rusia oleg tinkov pada senin 31 10 2022 aku telah lepas warga negara rusianya karena konflik di ukraina HTTPURL @USER\n",
      "Scenario 3: :astonished_face::astonished_face: miliarder rusia oleg tinkov pada senin (31/10/2022), mengaku telah  melepaskan kewarganegaraan rusianya karena konflik di ukraina.\n",
      "Scenario 4: :astonished_face::astonished_face: miliarder rusia oleg tinkov pada senin 31 10 2022 aku telah lepas warga negara rusianya karena konflik di ukraina\n",
      "Scenario 5: miliarder rusia oleg tinkov pada senin (31/10/2022), mengaku telah  melepaskan kewarganegaraan rusianya karena konflik di ukraina.\n",
      "Scenario 6: miliarder rusia oleg tinkov pada senin 31 10 2022 aku telah lepas warga negara rusianya karena konflik di ukraina\n"
     ]
    }
   ],
   "source": [
    "from kaelib.processor.TextProcessingPipeline import TextProcessingPipeline\n",
    "import kaelib.processor.preprocessing_func as pf\n",
    "\n",
    "scenario_processor = {\n",
    "    1: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.change_user,\n",
    "        pf.change_emoji,\n",
    "        pf.change_web_url\n",
    "    ]),\n",
    "    2: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.change_user,\n",
    "        pf.change_emoji,\n",
    "        pf.change_web_url,\n",
    "        stemmer.stem\n",
    "    ]),\n",
    "    3: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.remove_username,\n",
    "        pf.remove_url,\n",
    "        pf.change_emoji,\n",
    "    ]),\n",
    "    4: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.remove_username,\n",
    "        pf.remove_url,\n",
    "        pf.change_emoji,\n",
    "        stemmer.stem\n",
    "    ]),\n",
    "    5: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.remove_username,\n",
    "        pf.remove_url,\n",
    "        pf.remove_emoji,\n",
    "    ]),\n",
    "    6: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.remove_username,\n",
    "        pf.remove_url,\n",
    "        pf.remove_emoji,\n",
    "        stemmer.stem\n",
    "    ])\n",
    "}\n",
    "\n",
    "test_text = \"\"\"ðŸ˜²ðŸ˜² Miliarder Rusia Oleg Tinkov pada Senin (31/10/2022), mengaku telah  melepaskan kewarganegaraan Rusianya karena konflik di Ukraina. http://dlvr.it/Sc20gN @kaenova \"\"\"\n",
    "\n",
    "print(\"Example\")\n",
    "for i in scenario_processor:\n",
    "    print(f\"Scenario {i}:\", scenario_processor[i].process_text(test_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Organization and Replies from batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"../../data/3.1. Annotated\"\n",
    "save_path = \"../../data/3.2. Annotated Combined\"\n",
    "organization_file = [\n",
    "    \"organization_1.xlsx\",\n",
    "    \"organization_2.xlsx\",\n",
    "]\n",
    "replies_file = [\n",
    "    \"replies_100_1.xlsx\",\n",
    "    \"replies_100_2.xlsx\",\n",
    "]\n",
    "\n",
    "organization_df = pd.concat(\n",
    "    [\n",
    "        pd.read_excel(f\"{path}/{filename}\").replace(\n",
    "            {\"\\n\": \" \", \"\\r\": \"\", \"_x000D_\": \" \"}, regex=True\n",
    "        )\n",
    "        for filename in organization_file\n",
    "    ]\n",
    ")\n",
    "replies_df = pd.concat(\n",
    "    [\n",
    "        pd.read_excel(f\"{path}/{filename}\").replace(\n",
    "            {\"\\n\": \" \", \"\\r\": \"\", \"_x000D_\": \" \"}, regex=True\n",
    "        )\n",
    "        for filename in replies_file\n",
    "    ]\n",
    ")\n",
    "\n",
    "organization_df.to_csv(f\"{save_path}/organization.csv\", index=False)\n",
    "replies_df.to_csv(f\"{save_path}/replies.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Preprocess so that it has same data?\n",
    "1. Combine the csv from `organization.csv` and `replies.csv` in `3.2. Annotated Combined` folder\n",
    "2. Split it using `scikit-learn` with same random state to Train Test Validation\n",
    "3. With same source data, we preprocess it to diffrent preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "processed_path = \"../../data/4. Processed\"\n",
    "\n",
    "X_header_name = 'tweet'\n",
    "y_header_name = 'labels (Non-Headline 0 / Headline 1)'\n",
    "\n",
    "# Combine\n",
    "organization_combined_file = \"../../data/3.2. Annotated Combined/organization.csv\"\n",
    "organization_combined_df = pd.read_csv(organization_combined_file)[[X_header_name, y_header_name]]\n",
    "replies_combined_file = \"../../data/3.2. Annotated Combined/replies.csv\"\n",
    "replies_combined_df = pd.read_csv(replies_combined_file)[[X_header_name, y_header_name]]\n",
    "combined = pd.concat([organization_combined_df, replies_combined_df])\n",
    "\n",
    "# Split\n",
    "X = combined[X_header_name]\n",
    "y = combined[y_header_name]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2023, stratify=y) # 0.1 Test Data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=2023, stratify=y_train) # 0.25 x 0.9 = 0.225 Validation Data | 0.75 x 0.9 = 0.675 Training Data\n",
    "\n",
    "# Preprocess each scenario\n",
    "for scenario_number in scenario_processor:\n",
    "    scenario_save_path = f\"../../data/4. Processed/{scenario_number}\"\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        'tweet': X_train.map(lambda x: scenario_processor[scenario_number].process_text(x)),\n",
    "        'labels': y_train\n",
    "    })\n",
    "    \n",
    "    df_validation = pd.DataFrame({\n",
    "        'tweet': X_val.map(lambda x: scenario_processor[scenario_number].process_text(x)),\n",
    "        'labels': y_val\n",
    "    })\n",
    "    \n",
    "    df_test = pd.DataFrame({\n",
    "        'tweet': X_test.map(lambda x: scenario_processor[scenario_number].process_text(x)),\n",
    "        'labels': y_test\n",
    "    })\n",
    "    \n",
    "    if not os.path.exists(scenario_save_path):\n",
    "        os.mkdir(scenario_save_path)\n",
    "    \n",
    "    df_train.to_csv(f\"{scenario_save_path}/train.csv\", index=False, encoding='utf-8')\n",
    "    df_validation.to_csv(f\"{scenario_save_path}/validation.csv\", index=False, encoding='utf-8')\n",
    "    df_test.to_csv(f\"{scenario_save_path}/test.csv\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f859f1e6f927a99659bbed8c715d8b2e0ee62a381a7a8caee1b1f662fc1c2ad7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
