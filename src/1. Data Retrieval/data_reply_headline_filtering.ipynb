{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_no_reply = pd.read_csv(\"../../data/1. Raw/headline_no_reply_tweet_id.csv\")\n",
    "tweet_id_no_reply = {}\n",
    "for i in headline_no_reply.columns:\n",
    "    tweet_id_no_reply[i] = headline_no_reply[i].values\n",
    "\n",
    "files = list(tweet_id_no_reply.keys())\n",
    "\n",
    "# Removing -1\n",
    "for keys in tweet_id_no_reply:\n",
    "    tweet_id_no_reply[keys] = np.delete(tweet_id_no_reply[keys], np.where(tweet_id_no_reply[keys] == -1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_path = \"../../data/1. Raw/organization_3\"\n",
    "reply_path = \"../../data/1. Raw/replies\"\n",
    "\n",
    "headline_file = os.listdir(headline_path)\n",
    "reply_file = os.listdir(reply_path)\n",
    "\n",
    "headline_data = {}\n",
    "reply_data = {}\n",
    "\n",
    "for i in headline_file:\n",
    "    if i not in reply_file:\n",
    "        raise ValueError(f\"file {i} doesnt exist in reply path\")\n",
    "    headline_data[i] = pd.read_csv(f\"{headline_path}/{i}\")\n",
    "    reply_data[i] = pd.read_csv(f\"{reply_path}/{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Headline Tweet that the reply dont get crawled\n",
    "for keys in headline_data:\n",
    "    drop_idx = tweet_id_no_reply[keys]\n",
    "    headline_data[keys] = headline_data[keys].set_index('tweet_id').drop(drop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_headline_filtered_path = \"../../data/2. Filtered/organization\"\n",
    "\n",
    "# Save csv\n",
    "for keys in headline_data:\n",
    "    headline_data[keys].to_csv(f\"{save_headline_filtered_path}/{keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Reply Tweet that the headline dont get crawled\n",
    "for keys in reply_data:\n",
    "    get_idx = headline_data[keys].index.values\n",
    "    reply_data[keys] = reply_data[keys].set_index('tweet_conversation_id', drop=False).loc[get_idx].set_index('tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_replies_filtered_path = \"../../data/2. Filtered/replies\"\n",
    "\n",
    "# Save csv\n",
    "for keys in reply_data:\n",
    "    reply_data[keys].to_csv(f\"{save_replies_filtered_path}/{keys}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f859f1e6f927a99659bbed8c715d8b2e0ee62a381a7a8caee1b1f662fc1c2ad7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
