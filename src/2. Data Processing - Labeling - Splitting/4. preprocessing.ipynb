{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Notebooks\n",
    "\n",
    "Beberapa preprocessing untuk membuat skenario sebagai penelitian\n",
    "\n",
    "Preprocessing sesuai dengan IndoBERTweet\n",
    "1. Sama seperti preprosesing unutk membuat IndoBERTweet (lowercasing, change user, change emoji, change http url) (scenario 1)\n",
    "2. Nomor 1 dan melakukan stemming (scenario 2)\n",
    "\n",
    "Preprocessing untuk only full text tanpa adanya Twitter atribute  \n",
    "1. Lowecasing, Menghilangkan atribut url dan mention dalam text, change emoji  (scenario 3)\n",
    "2. Nomor 3 dan melakukan stemming (scenario 4)\n",
    "\n",
    "Preprocessing untuk only full text tanpa adanya Twitter attribute dan emoji agar hanya full text yang mirip dengan headline di berita-berita\n",
    "1. Lowecasing, Menghilangkan atribut url dan mention dalam text, menghilangkan emoji. (scenario 5)\n",
    "2. Nomor 5 dan melakukan stemming (scenario 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate some processing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NDETCStemmer import NDETCStemmer, CustomModelDownloader\n",
    "from kaelib.processor.NDETCStemmerWraper import NDETCStemmerWraper\n",
    "\n",
    "downloader = CustomModelDownloader(\n",
    "    model_1=\"http://127.0.0.1:5500/w2vec_wiki_id_case\",\n",
    "    model_2=\"http://127.0.0.1:5500/w2vec_wiki_id_case.trainables.syn1neg.npy\",\n",
    "    model_3=\"http://127.0.0.1:5500/w2vec_wiki_id_case.wv.vectors.npy\"\n",
    ")\n",
    "\n",
    "original_stemmer=NDETCStemmer(custom_downloader=downloader)\n",
    "stemmer = NDETCStemmerWraper(original_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n",
      "Scenario 1: :astonished_face::astonished_face: miliarder rusia oleg tinkov pada senin (31/10/2022), mengaku telah  melepaskan kewarganegaraan rusianya karena konflik di ukraina. HTTPURL @USER\n",
      "Scenario 2: :astonished_face::astonished_face: miliarder rusia oleg tinkov pada senin 31 10 2022 aku telah lepas warga negara rusianya karena konflik di ukraina HTTPURL @USER\n",
      "Scenario 3: :astonished_face::astonished_face: miliarder rusia oleg tinkov pada senin (31/10/2022), mengaku telah  melepaskan kewarganegaraan rusianya karena konflik di ukraina.\n",
      "Scenario 4: :astonished_face::astonished_face: miliarder rusia oleg tinkov pada senin 31 10 2022 aku telah lepas warga negara rusianya karena konflik di ukraina\n",
      "Scenario 5: miliarder rusia oleg tinkov pada senin (31/10/2022), mengaku telah  melepaskan kewarganegaraan rusianya karena konflik di ukraina.\n",
      "Scenario 6: miliarder rusia oleg tinkov pada senin 31 10 2022 aku telah lepas warga negara rusianya karena konflik di ukraina\n"
     ]
    }
   ],
   "source": [
    "from kaelib.processor.TextProcessingPipeline import TextProcessingPipeline\n",
    "import kaelib.processor.preprocessing_func as pf\n",
    "\n",
    "scenario_processor = {\n",
    "    1: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.change_user,\n",
    "        pf.change_emoji,\n",
    "        pf.change_web_url\n",
    "    ]),\n",
    "    2: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.change_user,\n",
    "        pf.change_emoji,\n",
    "        pf.change_web_url,\n",
    "        stemmer.stem\n",
    "    ]),\n",
    "    3: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.remove_username,\n",
    "        pf.remove_url,\n",
    "        pf.change_emoji,\n",
    "    ]),\n",
    "    4: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.remove_username,\n",
    "        pf.remove_url,\n",
    "        pf.change_emoji,\n",
    "        stemmer.stem\n",
    "    ]),\n",
    "    5: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.remove_username,\n",
    "        pf.remove_url,\n",
    "        pf.remove_emoji,\n",
    "    ]),\n",
    "    6: TextProcessingPipeline([\n",
    "        pf.lowercasing,\n",
    "        pf.remove_username,\n",
    "        pf.remove_url,\n",
    "        pf.remove_emoji,\n",
    "        stemmer.stem\n",
    "    ])\n",
    "}\n",
    "\n",
    "test_text = \"\"\"ðŸ˜²ðŸ˜² Miliarder Rusia Oleg Tinkov pada Senin (31/10/2022), mengaku telah  melepaskan kewarganegaraan Rusianya karena konflik di Ukraina. http://dlvr.it/Sc20gN @kaenova \"\"\"\n",
    "\n",
    "print(\"Example\")\n",
    "for i in scenario_processor:\n",
    "    print(f\"Scenario {i}:\", scenario_processor[i].process_text(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Preprocess so that it has same data?\n",
    "1. Combine the csv from `organization.csv` and `replies.csv` in `3.2. Annotated Combined` folder\n",
    "2. Split it using `scikit-learn` with same random state to Train Test Validation\n",
    "3. With same source data, we preprocess it to diffrent preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "processed_path = \"../../data/4. Processed\"\n",
    "\n",
    "X_header_name = 'tweet'\n",
    "y_header_name = 'labels (Non-Headline 0 / Headline 1)'\n",
    "\n",
    "# Combine\n",
    "organization_combined_file = \"../../data/3.2. Annotated Combined/organization.csv\"\n",
    "organization_combined_df = pd.read_csv(organization_combined_file)[[X_header_name, y_header_name]]\n",
    "replies_combined_file = \"../../data/3.2. Annotated Combined/replies.csv\"\n",
    "replies_combined_df = pd.read_csv(replies_combined_file)[[X_header_name, y_header_name]]\n",
    "combined = pd.concat([organization_combined_df, replies_combined_df])\n",
    "\n",
    "# Split\n",
    "X = combined[X_header_name]\n",
    "y = combined[y_header_name]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=2023, stratify=y) # 0.1 Test Data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=2023, stratify=y_train) # 0.15 x 0.9 = 0.135 Validation Data | 0.85 x 0.9 = 0.765 Training Data\n",
    "\n",
    "# Preprocess each scenario\n",
    "for scenario_number in scenario_processor:\n",
    "    scenario_save_path = f\"../../data/4. Processed/{scenario_number}\"\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        'tweet': X_train.map(lambda x: scenario_processor[scenario_number].process_text(x)),\n",
    "        'labels': y_train\n",
    "    })\n",
    "    \n",
    "    # df_validation = pd.DataFrame({\n",
    "    #     'tweet': X_val.map(lambda x: scenario_processor[scenario_number].process_text(x)),\n",
    "    #     'labels': y_val\n",
    "    # })\n",
    "    \n",
    "    df_test = pd.DataFrame({\n",
    "        'tweet': X_test.map(lambda x: scenario_processor[scenario_number].process_text(x)),\n",
    "        'labels': y_test\n",
    "    })\n",
    "    \n",
    "    if not os.path.exists(scenario_save_path):\n",
    "        os.mkdir(scenario_save_path)\n",
    "    \n",
    "    df_train.to_csv(f\"{scenario_save_path}/train.csv\", index=False, encoding='utf-8')\n",
    "    # df_validation.to_csv(f\"{scenario_save_path}/validation.csv\", index=False, encoding='utf-8')\n",
    "    df_test.to_csv(f\"{scenario_save_path}/test.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check number of labels on train valiudation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels (Non-Headline 0 / Headline 1)\n",
       "0    768\n",
       "1    755\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels (Non-Headline 0 / Headline 1)\n",
       "0    192\n",
       "1    189\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f859f1e6f927a99659bbed8c715d8b2e0ee62a381a7a8caee1b1f662fc1c2ad7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
