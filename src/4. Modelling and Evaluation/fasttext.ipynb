{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Prepare Google Colab Environement and build handmade library\n",
    "# !git clone https://github.com/kaenova/Headline_Detection.git\n",
    "# %cd \"/content/Headline_Detection\"\n",
    "\n",
    "# !make lib\n",
    "\n",
    "# %cd \"/content/\"\n",
    "\n",
    "# print(\"Please upload '4. Processed.zip'\")\n",
    "# from google.colab import files\n",
    "# files.upload()\n",
    "\n",
    "# !unzip \"/content/4. Processed.zip\"\n",
    "\n",
    "# print(\"Please upload .env\")\n",
    "# from google.colab import files\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Reset Google Colab Environment\n",
    "# %cd ..\n",
    "# !rm -fr Headline_Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Config and Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load env for S3 uploads\n",
    "from dotenv import load_dotenv\n",
    "from kaelib.s3 import check_required_env_vars\n",
    "import os\n",
    "load_dotenv()\n",
    "check_required_env_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "environment = 'local'\n",
    "if environment == 'local':\n",
    "    data_path = \"../../data/4. Processed/\"\n",
    "elif environment == 'colab':\n",
    "    data_path = \"/content/4. Processed/\"\n",
    "else:\n",
    "    raise ImportError(\"Data environment\")\n",
    "\n",
    "\n",
    "folders = os.listdir(data_path)\n",
    "scenario_data = {}\n",
    "\n",
    "for folder in folders:\n",
    "    current_data_path = f\"{data_path}/{folder}\"\n",
    "    if not os.path.isdir(current_data_path):\n",
    "        continue\n",
    "    files = os.listdir(current_data_path)\n",
    "    data_dict = {}\n",
    "    for file in files:\n",
    "        current_file = f\"{current_data_path}/{file}\"\n",
    "        data_dict[file] = pd.read_csv(current_file)\n",
    "    scenario_data[int(folder)] = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas\n",
    "import typing\n",
    "from torch.utils.data import Dataset\n",
    "from kaelib.processor import TextProcessingPipeline\n",
    "\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: \"pandas.DataFrame\",\n",
    "        x_column_name: \"str\" = \"tweet\",\n",
    "        y_column_name: \"str\" = \"labels\",\n",
    "        preprocessor: \"typing.Optional[TextProcessingPipeline]\" = None,\n",
    "    ):\n",
    "        self.x = df[x_column_name].astype(str).to_list()\n",
    "        self.y = torch.tensor(df[y_column_name].astype(int).to_list())\n",
    "        assert len(self.x) == len(self.y)\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def _process_idx_text(self, idx):\n",
    "        data = self.x[idx]\n",
    "        if type(idx) is not slice:\n",
    "            data = [self.x[idx]]\n",
    "        if self.preprocessor is not None:\n",
    "            data = self.preprocessor.process_corpus(data)\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        processed_corpus = self._process_idx_text(idx)\n",
    "        return processed_corpus, self.y[idx]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"\\n\".join([f\"{self.x[i]} : {self.y[i]}\" for i in range(5)])\n",
    "\n",
    "\n",
    "scenario_datasets = {}\n",
    "for scenario in scenario_data:\n",
    "    data = scenario_data[scenario]\n",
    "    datasets = {}\n",
    "    for data_type in data:\n",
    "        datasets[data_type] = TextClassificationDataset(data[data_type])\n",
    "    scenario_datasets[scenario] = datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyper_params = {\n",
    "    'seq_length': 256,\n",
    "    'learning_rate': 1e-5,\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1607.01759.pdf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchnlp.word_to_vector import FastText\n",
    "import lightning.pytorch as pl\n",
    "from torchmetrics.classification import F1Score, Accuracy, Recall, Precision\n",
    "\n",
    "\n",
    "class FastTextClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: \"int\" = 256,\n",
    "        out_feature: \"int\" = 2,\n",
    "        pad_sequence: \"bool\" = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.pad_sequence = pad_sequence\n",
    "        self.fasttext = FastText(\"id\")\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(300, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_feature),\n",
    "        )\n",
    "        self.f1_scorer = F1Score(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.accuracy_scorer = Accuracy(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.precision_scorer = Precision(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.recall_scorer = Recall(task=\"multiclass\", num_classes=out_feature)\n",
    "\n",
    "    def _forward_fasttext(self, x: \"list[str]\"):\n",
    "        batch_text_embedding = torch.tensor([]).to(self.device)\n",
    "        for sentence in x:\n",
    "            sentence_seq = sentence.split(\" \")\n",
    "            if len(sentence_seq) > self.seq_length:\n",
    "                sentence_seq = sentence_seq[: self.seq_length]\n",
    "            if self.pad_sequence:\n",
    "                while len(sentence_seq) < 256:\n",
    "                    sentence_seq.append(\"<pad>\")\n",
    "            word_embedding = (\n",
    "                self.fasttext[sentence_seq].mean(dim=0).unsqueeze(0).to(self.device)\n",
    "            )\n",
    "            batch_text_embedding = torch.cat((batch_text_embedding, word_embedding))\n",
    "        return batch_text_embedding\n",
    "\n",
    "    def forward(self, x: \"list[str]\") -> \"torch.Tensor\":\n",
    "        # Prepare str\n",
    "        logits: torch.Tensor = self._forward_fasttext(x)\n",
    "        logits = self.feed_forward(logits)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"training_loss\", loss)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"training_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"training_accuracy\", accuracy, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"validation_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"validation_accuracy\", accuracy, prog_bar=True)\n",
    "        precision = self.precision_scorer(pred, y)\n",
    "        self.log(\"validation_precision\", precision, prog_bar=True)\n",
    "        recall = self.recall_scorer(pred, y)\n",
    "        self.log(\"validation_recall\", recall, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"test_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"test_accuracy\", accuracy, prog_bar=True)\n",
    "        precision = self.precision_scorer(pred, y)\n",
    "        self.log(\"test_precision\", precision, prog_bar=True)\n",
    "        recall = self.recall_scorer(pred, y)\n",
    "        self.log(\"test_recall\", recall, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=hyper_params['learning_rate'])\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_module = FastTextClassifier\n",
    "def create_model():\n",
    "    return model_module(hyper_params['seq_length'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model(['test','halo']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Val, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "available_cpus = 0\n",
    "model_name = \"fasttext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name             | Type                | Params\n",
      "---------------------------------------------------------\n",
      "0 | feed_forward     | Sequential          | 54.9 K\n",
      "1 | f1_scorer        | MulticlassF1Score   | 0     \n",
      "2 | accuracy_scorer  | MulticlassAccuracy  | 0     \n",
      "3 | precision_scorer | MulticlassPrecision | 0     \n",
      "4 | recall_scorer    | MulticlassRecall    | 0     \n",
      "---------------------------------------------------------\n",
      "54.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "54.9 K    Total params\n",
      "0.220     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee03301278024777acacf77fd7fd1b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "for scenario in scenario_datasets:\n",
    "    np.random.seed(2023)\n",
    "    random.seed(2023)\n",
    "    torch.manual_seed(2023)\n",
    "\n",
    "    run_id = f\"{timestamp}_scenario_{scenario}\"\n",
    "\n",
    "    if scenario in [0]:\n",
    "        continue\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    # Dataset\n",
    "    train_dataset = scenario_datasets[scenario][\"train.csv\"]\n",
    "    test_dataset = scenario_datasets[scenario][\"test.csv\"]\n",
    "    validation_dataset = scenario_datasets[scenario][\"validation.csv\"]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=hyper_params['batch_size'], num_workers=available_cpus)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=hyper_params['batch_size'], num_workers=available_cpus)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=hyper_params['batch_size'], num_workers=available_cpus)\n",
    "\n",
    "    tb_log = TensorBoardLogger(\"tensorboard\", model_name, run_id)\n",
    "    tb_log.log_hyperparams(hyper_params)\n",
    "\n",
    "    checkpoint_callback_val = ModelCheckpoint(monitor=\"validation_loss\", filename='val_{epoch}-{validation_loss:.4f}')\n",
    "    checkpoint_callback_train = ModelCheckpoint(monitor=\"training_loss\", filename='train_{epoch}-{training_loss:.4f}')\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"validation_loss\", min_delta=1e-3, patience=5, mode=\"min\"\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=tb_log,\n",
    "        log_every_n_steps=2, \n",
    "        callbacks=[early_stop_callback, checkpoint_callback_val, checkpoint_callback_train]\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader\n",
    "    )\n",
    "    model = model_module.load_from_checkpoint(checkpoint_callback_val.best_model_path)\n",
    "    trainer.test(model=model, dataloaders=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    " 0 : \"Bukan headline\",\n",
    " 1 : \"Headline\"\n",
    "}\n",
    "\n",
    "test_input = [\n",
    "    \"aku suka aplikasi ini\", \n",
    "    \"tidak suka sama aplikasi ini\", \n",
    "    \"oke\", \n",
    "    \"keren aplikasi\", \n",
    "    \"pengungkapan itu ditandai dengan ditangkapnya 4 orang pria, dan ditemukan sabu dalam klip plastik bening saat digeledah\"\n",
    "]\n",
    "with torch.no_grad():\n",
    "    pred = model(test_input)\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    pred_np = pred.argmax(dim=1).cpu().detach().numpy()\n",
    "    for i in range(len(test_input)):\n",
    "        print(f\"'{test_input[i]}' : {id2label[pred_np[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Logging and Checkpoint to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Uploading:   0%|                                                                             | 0/100 [00:00<?, ?file/s]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   0%| | 0/100 [00:00<?, ?file/s, file=2023-04-12_11-47-02_scenario_1\\events.out.tfevents.1681274823.PCRumah.\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   1%| | 1/100 [00:00<00:25,  3.83file/s, file=2023-04-12_11-47-02_scenario_1\\events.out.tfevents.1681274823.\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   1%| | 1/100 [00:00<00:25,  3.83file/s, file=2023-04-12_11-47-02_scenario_1\\events.out.tfevents.1681274919.\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   2%|▍                  | 2/100 [00:00<00:25,  3.83file/s, file=2023-04-12_11-47-02_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   3%|▌                  | 3/100 [00:00<00:12,  7.53file/s, file=2023-04-12_11-47-02_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   3%| | 3/100 [00:01<00:12,  7.53file/s, file=2023-04-12_11-47-02_scenario_1\\checkpoints\\train_epoch=356-tra\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   4%| | 4/100 [00:01<00:33,  2.84file/s, file=2023-04-12_11-47-02_scenario_1\\checkpoints\\train_epoch=356-tra\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   4%| | 4/100 [00:01<00:33,  2.84file/s, file=2023-04-12_11-47-02_scenario_1\\checkpoints\\val_epoch=356-valid\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   5%| | 5/100 [00:01<00:40,  2.33file/s, file=2023-04-12_11-47-02_scenario_1\\checkpoints\\val_epoch=356-valid\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   5%| | 5/100 [00:01<00:40,  2.33file/s, file=2023-04-12_11-47-02_scenario_2\\events.out.tfevents.1681274920.\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   6%| | 6/100 [00:01<00:31,  2.94file/s, file=2023-04-12_11-47-02_scenario_2\\events.out.tfevents.1681274920.\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   6%| | 6/100 [00:01<00:31,  2.94file/s, file=2023-04-12_11-47-02_scenario_2\\events.out.tfevents.1681274999.\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   7%|█▎                 | 7/100 [00:02<00:31,  2.94file/s, file=2023-04-12_11-47-02_scenario_2\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   8%| | 8/100 [00:02<00:31,  2.94file/s, file=2023-04-12_11-47-02_scenario_2\\checkpoints\\train_epoch=313-tra\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   9%| | 9/100 [00:02<00:24,  3.73file/s, file=2023-04-12_11-47-02_scenario_2\\checkpoints\\train_epoch=313-tra\u001b[A\u001b[A\n",
      "\n",
      "Uploading:   9%| | 9/100 [00:03<00:24,  3.73file/s, file=2023-04-12_11-47-02_scenario_2\\checkpoints\\val_epoch=313-valid\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  10%| | 10/100 [00:03<00:31,  2.88file/s, file=2023-04-12_11-47-02_scenario_2\\checkpoints\\val_epoch=313-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  10%| | 10/100 [00:03<00:31,  2.88file/s, file=2023-04-12_11-47-02_scenario_3\\events.out.tfevents.1681275000\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  11%| | 11/100 [00:03<00:26,  3.30file/s, file=2023-04-12_11-47-02_scenario_3\\events.out.tfevents.1681275000\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  11%| | 11/100 [00:03<00:26,  3.30file/s, file=2023-04-12_11-47-02_scenario_3\\events.out.tfevents.1681275094\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  12%| | 12/100 [00:03<00:25,  3.42file/s, file=2023-04-12_11-47-02_scenario_3\\events.out.tfevents.1681275094\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  12%|██▏               | 12/100 [00:03<00:25,  3.42file/s, file=2023-04-12_11-47-02_scenario_3\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  13%|▏| 13/100 [00:04<00:25,  3.42file/s, file=2023-04-12_11-47-02_scenario_3\\checkpoints\\train_epoch=348-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  14%|▏| 14/100 [00:04<00:25,  3.38file/s, file=2023-04-12_11-47-02_scenario_3\\checkpoints\\train_epoch=348-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  14%|▏| 14/100 [00:04<00:25,  3.38file/s, file=2023-04-12_11-47-02_scenario_3\\checkpoints\\val_epoch=348-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  15%|▏| 15/100 [00:04<00:30,  2.76file/s, file=2023-04-12_11-47-02_scenario_3\\checkpoints\\val_epoch=348-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  15%|▏| 15/100 [00:04<00:30,  2.76file/s, file=2023-04-12_11-47-02_scenario_4\\events.out.tfevents.1681275095\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  16%|▏| 16/100 [00:04<00:25,  3.27file/s, file=2023-04-12_11-47-02_scenario_4\\events.out.tfevents.1681275095\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  16%|▏| 16/100 [00:04<00:25,  3.27file/s, file=2023-04-12_11-47-02_scenario_4\\events.out.tfevents.1681275174\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  17%|███               | 17/100 [00:05<00:25,  3.27file/s, file=2023-04-12_11-47-02_scenario_4\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  18%|▏| 18/100 [00:05<00:25,  3.27file/s, file=2023-04-12_11-47-02_scenario_4\\checkpoints\\train_epoch=298-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  19%|▏| 19/100 [00:05<00:21,  3.77file/s, file=2023-04-12_11-47-02_scenario_4\\checkpoints\\train_epoch=298-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  19%|▏| 19/100 [00:06<00:21,  3.77file/s, file=2023-04-12_11-47-02_scenario_4\\checkpoints\\val_epoch=298-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  20%|▏| 20/100 [00:06<00:27,  2.95file/s, file=2023-04-12_11-47-02_scenario_4\\checkpoints\\val_epoch=298-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  20%|▏| 20/100 [00:06<00:27,  2.95file/s, file=2023-04-12_11-47-02_scenario_5\\events.out.tfevents.1681275175\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  21%|▏| 21/100 [00:06<00:24,  3.29file/s, file=2023-04-12_11-47-02_scenario_5\\events.out.tfevents.1681275175\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  21%|▏| 21/100 [00:06<00:24,  3.29file/s, file=2023-04-12_11-47-02_scenario_5\\events.out.tfevents.1681275268\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  22%|███▉              | 22/100 [00:06<00:23,  3.29file/s, file=2023-04-12_11-47-02_scenario_5\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  23%|▏| 23/100 [00:07<00:23,  3.29file/s, file=2023-04-12_11-47-02_scenario_5\\checkpoints\\train_epoch=349-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  24%|▏| 24/100 [00:07<00:25,  2.95file/s, file=2023-04-12_11-47-02_scenario_5\\checkpoints\\train_epoch=349-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  24%|▏| 24/100 [00:08<00:25,  2.95file/s, file=2023-04-12_11-47-02_scenario_5\\checkpoints\\val_epoch=349-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  25%|▎| 25/100 [00:08<00:30,  2.46file/s, file=2023-04-12_11-47-02_scenario_5\\checkpoints\\val_epoch=349-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  25%|▎| 25/100 [00:08<00:30,  2.46file/s, file=2023-04-12_11-47-02_scenario_6\\events.out.tfevents.1681275269\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  26%|▎| 26/100 [00:08<00:26,  2.84file/s, file=2023-04-12_11-47-02_scenario_6\\events.out.tfevents.1681275269\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  26%|▎| 26/100 [00:08<00:26,  2.84file/s, file=2023-04-12_11-47-02_scenario_6\\events.out.tfevents.1681275350\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  27%|▎| 27/100 [00:08<00:24,  2.99file/s, file=2023-04-12_11-47-02_scenario_6\\events.out.tfevents.1681275350\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  27%|████▊             | 27/100 [00:08<00:24,  2.99file/s, file=2023-04-12_11-47-02_scenario_6\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  28%|▎| 28/100 [00:09<00:24,  2.99file/s, file=2023-04-12_11-47-02_scenario_6\\checkpoints\\train_epoch=302-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  29%|▎| 29/100 [00:09<00:29,  2.40file/s, file=2023-04-12_11-47-02_scenario_6\\checkpoints\\train_epoch=302-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  29%|▎| 29/100 [00:10<00:29,  2.40file/s, file=2023-04-12_11-47-02_scenario_6\\checkpoints\\val_epoch=302-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  30%|▎| 30/100 [00:10<00:37,  1.89file/s, file=2023-04-12_11-47-02_scenario_6\\checkpoints\\val_epoch=302-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  30%|▎| 30/100 [00:11<00:37,  1.89file/s, file=2023-04-14_19-38-06_scenario_1\\events.out.tfevents.1681475887\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  31%|▎| 31/100 [00:11<00:39,  1.76file/s, file=2023-04-14_19-38-06_scenario_1\\events.out.tfevents.1681475887\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  31%|▎| 31/100 [00:11<00:39,  1.76file/s, file=2023-04-14_19-38-06_scenario_1\\events.out.tfevents.1681476021\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  32%|█████▊            | 32/100 [00:11<00:38,  1.76file/s, file=2023-04-14_19-38-06_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  33%|▎| 33/100 [00:12<00:38,  1.76file/s, file=2023-04-14_19-38-06_scenario_1\\checkpoints\\train_epoch=263-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  34%|▎| 34/100 [00:12<00:26,  2.46file/s, file=2023-04-14_19-38-06_scenario_1\\checkpoints\\train_epoch=263-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  34%|▎| 34/100 [00:12<00:26,  2.46file/s, file=2023-04-14_19-38-06_scenario_1\\checkpoints\\val_epoch=263-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  35%|▎| 35/100 [00:12<00:28,  2.28file/s, file=2023-04-14_19-38-06_scenario_1\\checkpoints\\val_epoch=263-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  35%|▎| 35/100 [00:13<00:28,  2.28file/s, file=2023-04-14_19-38-06_scenario_2\\events.out.tfevents.1681476022\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  36%|▎| 36/100 [00:13<00:31,  2.03file/s, file=2023-04-14_19-38-06_scenario_2\\events.out.tfevents.1681476022\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  36%|▎| 36/100 [00:13<00:31,  2.03file/s, file=2023-04-14_19-38-06_scenario_2\\events.out.tfevents.1681476158\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  37%|▎| 37/100 [00:13<00:25,  2.48file/s, file=2023-04-14_19-38-06_scenario_2\\events.out.tfevents.1681476158\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  37%|██████▋           | 37/100 [00:13<00:25,  2.48file/s, file=2023-04-14_19-38-06_scenario_2\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  38%|▍| 38/100 [00:14<00:25,  2.48file/s, file=2023-04-14_19-38-06_scenario_2\\checkpoints\\train_epoch=267-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  39%|▍| 39/100 [00:14<00:23,  2.61file/s, file=2023-04-14_19-38-06_scenario_2\\checkpoints\\train_epoch=267-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  39%|▍| 39/100 [00:14<00:23,  2.61file/s, file=2023-04-14_19-38-06_scenario_2\\checkpoints\\val_epoch=267-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  40%|▍| 40/100 [00:14<00:25,  2.31file/s, file=2023-04-14_19-38-06_scenario_2\\checkpoints\\val_epoch=267-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  40%|▍| 40/100 [00:15<00:25,  2.31file/s, file=2023-04-14_19-38-06_scenario_3\\events.out.tfevents.1681476159\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  41%|▍| 41/100 [00:15<00:29,  2.02file/s, file=2023-04-14_19-38-06_scenario_3\\events.out.tfevents.1681476159\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  41%|▍| 41/100 [00:15<00:29,  2.02file/s, file=2023-04-14_19-38-06_scenario_3\\events.out.tfevents.1681476307\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  42%|███████▌          | 42/100 [00:15<00:28,  2.02file/s, file=2023-04-14_19-38-06_scenario_3\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  43%|▍| 43/100 [00:16<00:28,  2.02file/s, file=2023-04-14_19-38-06_scenario_3\\checkpoints\\train_epoch=272-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  44%|▍| 44/100 [00:16<00:19,  2.80file/s, file=2023-04-14_19-38-06_scenario_3\\checkpoints\\train_epoch=272-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  44%|▍| 44/100 [00:16<00:19,  2.80file/s, file=2023-04-14_19-38-06_scenario_3\\checkpoints\\val_epoch=272-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  45%|▍| 45/100 [00:16<00:22,  2.47file/s, file=2023-04-14_19-38-06_scenario_3\\checkpoints\\val_epoch=272-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  45%|▍| 45/100 [00:17<00:22,  2.47file/s, file=2023-04-14_19-38-06_scenario_4\\events.out.tfevents.1681476308\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  46%|▍| 46/100 [00:17<00:25,  2.09file/s, file=2023-04-14_19-38-06_scenario_4\\events.out.tfevents.1681476308\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  46%|▍| 46/100 [00:17<00:25,  2.09file/s, file=2023-04-14_19-38-06_scenario_4\\events.out.tfevents.1681476462\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  47%|▍| 47/100 [00:17<00:22,  2.41file/s, file=2023-04-14_19-38-06_scenario_4\\events.out.tfevents.1681476462\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  47%|████████▍         | 47/100 [00:17<00:22,  2.41file/s, file=2023-04-14_19-38-06_scenario_4\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  48%|▍| 48/100 [00:18<00:21,  2.41file/s, file=2023-04-14_19-38-06_scenario_4\\checkpoints\\train_epoch=281-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  49%|▍| 49/100 [00:18<00:20,  2.48file/s, file=2023-04-14_19-38-06_scenario_4\\checkpoints\\train_epoch=281-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  49%|▍| 49/100 [00:19<00:20,  2.48file/s, file=2023-04-14_19-38-06_scenario_4\\checkpoints\\val_epoch=281-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  50%|▌| 50/100 [00:19<00:22,  2.23file/s, file=2023-04-14_19-38-06_scenario_4\\checkpoints\\val_epoch=281-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  50%|▌| 50/100 [00:20<00:22,  2.23file/s, file=2023-04-14_19-38-06_scenario_5\\events.out.tfevents.1681476463\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  51%|▌| 51/100 [00:20<00:27,  1.78file/s, file=2023-04-14_19-38-06_scenario_5\\events.out.tfevents.1681476463\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  51%|▌| 51/100 [00:20<00:27,  1.78file/s, file=2023-04-14_19-38-06_scenario_5\\events.out.tfevents.1681476613\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  52%|█████████▎        | 52/100 [00:20<00:27,  1.78file/s, file=2023-04-14_19-38-06_scenario_5\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  53%|█████████▌        | 53/100 [00:20<00:16,  2.82file/s, file=2023-04-14_19-38-06_scenario_5\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  53%|▌| 53/100 [00:20<00:16,  2.82file/s, file=2023-04-14_19-38-06_scenario_5\\checkpoints\\train_epoch=272-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  54%|▌| 54/100 [00:20<00:18,  2.43file/s, file=2023-04-14_19-38-06_scenario_5\\checkpoints\\train_epoch=272-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  54%|▌| 54/100 [00:21<00:18,  2.43file/s, file=2023-04-14_19-38-06_scenario_5\\checkpoints\\val_epoch=272-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  55%|▌| 55/100 [00:21<00:20,  2.16file/s, file=2023-04-14_19-38-06_scenario_5\\checkpoints\\val_epoch=272-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  55%|▌| 55/100 [00:22<00:20,  2.16file/s, file=2023-04-14_19-38-06_scenario_6\\events.out.tfevents.1681476614\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  56%|▌| 56/100 [00:22<00:23,  1.86file/s, file=2023-04-14_19-38-06_scenario_6\\events.out.tfevents.1681476614\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  56%|▌| 56/100 [00:22<00:23,  1.86file/s, file=2023-04-14_19-38-06_scenario_6\\events.out.tfevents.1681476761\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  57%|▌| 57/100 [00:22<00:18,  2.34file/s, file=2023-04-14_19-38-06_scenario_6\\events.out.tfevents.1681476761\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  57%|██████████▎       | 57/100 [00:22<00:18,  2.34file/s, file=2023-04-14_19-38-06_scenario_6\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  58%|▌| 58/100 [00:22<00:17,  2.34file/s, file=2023-04-14_19-38-06_scenario_6\\checkpoints\\train_epoch=279-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  59%|▌| 59/100 [00:22<00:15,  2.64file/s, file=2023-04-14_19-38-06_scenario_6\\checkpoints\\train_epoch=279-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  59%|▌| 59/100 [00:23<00:15,  2.64file/s, file=2023-04-14_19-38-06_scenario_6\\checkpoints\\val_epoch=279-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  60%|▌| 60/100 [00:23<00:17,  2.31file/s, file=2023-04-14_19-38-06_scenario_6\\checkpoints\\val_epoch=279-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  60%|▌| 60/100 [00:23<00:17,  2.31file/s, file=2023-04-16_05-25-19_scenario_1\\events.out.tfevents.1681597521\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  61%|▌| 61/100 [00:23<00:14,  2.68file/s, file=2023-04-16_05-25-19_scenario_1\\events.out.tfevents.1681597521\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  61%|██████████▉       | 61/100 [00:23<00:14,  2.68file/s, file=2023-04-16_05-25-19_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  62%|▌| 62/100 [00:24<00:14,  2.68file/s, file=2023-04-16_05-25-19_scenario_1\\checkpoints\\train_epoch=58-tra\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  63%|▋| 63/100 [00:24<00:13,  2.80file/s, file=2023-04-16_05-25-19_scenario_1\\checkpoints\\train_epoch=58-tra\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  63%|▋| 63/100 [00:24<00:13,  2.80file/s, file=2023-04-16_05-25-19_scenario_1\\checkpoints\\val_epoch=58-valid\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  64%|▋| 64/100 [00:24<00:14,  2.42file/s, file=2023-04-16_05-25-19_scenario_1\\checkpoints\\val_epoch=58-valid\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  64%|▋| 64/100 [00:25<00:14,  2.42file/s, file=2023-04-16_05-34-38_scenario_1\\events.out.tfevents.1681598080\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  65%|▋| 65/100 [00:25<00:16,  2.08file/s, file=2023-04-16_05-34-38_scenario_1\\events.out.tfevents.1681598080\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  65%|▋| 65/100 [00:25<00:16,  2.08file/s, file=2023-04-16_05-34-38_scenario_1\\events.out.tfevents.1681598210\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  66%|███████████▉      | 66/100 [00:25<00:16,  2.08file/s, file=2023-04-16_05-34-38_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  67%|████████████      | 67/100 [00:25<00:10,  3.11file/s, file=2023-04-16_05-34-38_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  67%|▋| 67/100 [00:26<00:10,  3.11file/s, file=2023-04-16_05-34-38_scenario_1\\checkpoints\\train_epoch=263-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  68%|▋| 68/100 [00:26<00:12,  2.57file/s, file=2023-04-16_05-34-38_scenario_1\\checkpoints\\train_epoch=263-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  68%|▋| 68/100 [00:27<00:12,  2.57file/s, file=2023-04-16_05-34-38_scenario_1\\checkpoints\\val_epoch=263-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  69%|▋| 69/100 [00:27<00:13,  2.28file/s, file=2023-04-16_05-34-38_scenario_1\\checkpoints\\val_epoch=263-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  69%|▋| 69/100 [00:27<00:13,  2.28file/s, file=2023-04-16_05-34-38_scenario_2\\events.out.tfevents.1681598212\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  70%|▋| 70/100 [00:27<00:12,  2.34file/s, file=2023-04-16_05-34-38_scenario_2\\events.out.tfevents.1681598212\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  70%|████████████▌     | 70/100 [00:27<00:12,  2.34file/s, file=2023-04-16_05-34-38_scenario_2\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  71%|████████████▊     | 71/100 [00:27<00:09,  2.94file/s, file=2023-04-16_05-34-38_scenario_2\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  71%|▋| 71/100 [00:28<00:09,  2.94file/s, file=2023-04-16_05-34-38_scenario_2\\checkpoints\\train_epoch=143-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  72%|▋| 72/100 [00:28<00:11,  2.39file/s, file=2023-04-16_05-34-38_scenario_2\\checkpoints\\train_epoch=143-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  72%|▋| 72/100 [00:28<00:11,  2.39file/s, file=2023-04-16_05-34-38_scenario_2\\checkpoints\\val_epoch=143-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  73%|▋| 73/100 [00:28<00:12,  2.14file/s, file=2023-04-16_05-34-38_scenario_2\\checkpoints\\val_epoch=143-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  73%|▋| 73/100 [00:29<00:12,  2.14file/s, file=2023-04-17_20-12-03_scenario_1\\events.out.tfevents.1681737124\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  74%|▋| 74/100 [00:29<00:14,  1.80file/s, file=2023-04-17_20-12-03_scenario_1\\events.out.tfevents.1681737124\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  74%|▋| 74/100 [00:29<00:14,  1.80file/s, file=2023-04-17_20-12-03_scenario_1\\events.out.tfevents.1681737258\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  75%|█████████████▌    | 75/100 [00:29<00:13,  1.80file/s, file=2023-04-17_20-12-03_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  76%|▊| 76/100 [00:30<00:13,  1.80file/s, file=2023-04-17_20-12-03_scenario_1\\checkpoints\\train_epoch=263-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  77%|▊| 77/100 [00:30<00:08,  2.65file/s, file=2023-04-17_20-12-03_scenario_1\\checkpoints\\train_epoch=263-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  77%|▊| 77/100 [00:30<00:08,  2.65file/s, file=2023-04-17_20-12-03_scenario_1\\checkpoints\\val_epoch=263-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  78%|▊| 78/100 [00:30<00:09,  2.38file/s, file=2023-04-17_20-12-03_scenario_1\\checkpoints\\val_epoch=263-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  78%|▊| 78/100 [00:31<00:09,  2.38file/s, file=2023-04-17_20-12-03_scenario_2\\events.out.tfevents.1681737259\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  79%|▊| 79/100 [00:31<00:10,  2.08file/s, file=2023-04-17_20-12-03_scenario_2\\events.out.tfevents.1681737259\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  79%|▊| 79/100 [00:31<00:10,  2.08file/s, file=2023-04-17_20-12-03_scenario_2\\events.out.tfevents.1681737384\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  80%|██████████████▍   | 80/100 [00:31<00:09,  2.08file/s, file=2023-04-17_20-12-03_scenario_2\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  81%|▊| 81/100 [00:32<00:09,  2.08file/s, file=2023-04-17_20-12-03_scenario_2\\checkpoints\\train_epoch=267-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  82%|▊| 82/100 [00:32<00:06,  2.82file/s, file=2023-04-17_20-12-03_scenario_2\\checkpoints\\train_epoch=267-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  82%|▊| 82/100 [00:32<00:06,  2.82file/s, file=2023-04-17_20-12-03_scenario_2\\checkpoints\\val_epoch=267-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  83%|▊| 83/100 [00:32<00:06,  2.44file/s, file=2023-04-17_20-12-03_scenario_2\\checkpoints\\val_epoch=267-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  83%|▊| 83/100 [00:33<00:06,  2.44file/s, file=2023-04-17_20-12-03_scenario_3\\events.out.tfevents.1681737385\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  84%|▊| 84/100 [00:33<00:07,  2.00file/s, file=2023-04-17_20-12-03_scenario_3\\events.out.tfevents.1681737385\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  84%|▊| 84/100 [00:33<00:07,  2.00file/s, file=2023-04-17_20-12-03_scenario_3\\events.out.tfevents.1681737511\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  85%|███████████████▎  | 85/100 [00:33<00:07,  2.00file/s, file=2023-04-17_20-12-03_scenario_3\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  86%|███████████████▍  | 86/100 [00:33<00:05,  2.74file/s, file=2023-04-17_20-12-03_scenario_3\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  86%|▊| 86/100 [00:34<00:05,  2.74file/s, file=2023-04-17_20-12-03_scenario_3\\checkpoints\\train_epoch=272-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  87%|▊| 87/100 [00:34<00:05,  2.36file/s, file=2023-04-17_20-12-03_scenario_3\\checkpoints\\train_epoch=272-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  87%|▊| 87/100 [00:35<00:05,  2.36file/s, file=2023-04-17_20-12-03_scenario_3\\checkpoints\\val_epoch=272-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  88%|▉| 88/100 [00:35<00:05,  2.10file/s, file=2023-04-17_20-12-03_scenario_3\\checkpoints\\val_epoch=272-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  88%|▉| 88/100 [00:36<00:05,  2.10file/s, file=2023-04-17_20-12-03_scenario_4\\events.out.tfevents.1681737512\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  89%|▉| 89/100 [00:36<00:06,  1.82file/s, file=2023-04-17_20-12-03_scenario_4\\events.out.tfevents.1681737512\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  89%|████████████████  | 89/100 [00:36<00:06,  1.82file/s, file=2023-04-17_20-12-03_scenario_4\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  90%|▉| 90/100 [00:36<00:05,  1.82file/s, file=2023-04-17_20-12-03_scenario_4\\checkpoints\\train_epoch=264-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  91%|▉| 91/100 [00:36<00:04,  2.17file/s, file=2023-04-17_20-12-03_scenario_4\\checkpoints\\train_epoch=264-tr\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  91%|▉| 91/100 [00:37<00:04,  2.17file/s, file=2023-04-17_20-12-03_scenario_4\\checkpoints\\val_epoch=264-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  92%|▉| 92/100 [00:37<00:03,  2.01file/s, file=2023-04-17_20-12-03_scenario_4\\checkpoints\\val_epoch=264-vali\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  92%|▉| 92/100 [00:37<00:03,  2.01file/s, file=2023-04-17_20-20-46_scenario_1\\events.out.tfevents.1681737647\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  93%|████████████████▋ | 93/100 [00:37<00:03,  2.01file/s, file=2023-04-17_20-20-46_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  94%|████████████████▉ | 94/100 [00:37<00:01,  3.03file/s, file=2023-04-17_20-20-46_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  94%|▉| 94/100 [00:38<00:01,  3.03file/s, file=2023-04-17_20-20-46_scenario_1\\checkpoints\\train_epoch=14-tra\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  95%|▉| 95/100 [00:38<00:02,  2.29file/s, file=2023-04-17_20-20-46_scenario_1\\checkpoints\\train_epoch=14-tra\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  95%|▉| 95/100 [00:38<00:02,  2.29file/s, file=2023-04-17_20-20-46_scenario_1\\checkpoints\\val_epoch=14-valid\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  96%|▉| 96/100 [00:38<00:01,  2.09file/s, file=2023-04-17_20-20-46_scenario_1\\checkpoints\\val_epoch=14-valid\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  96%|▉| 96/100 [00:38<00:01,  2.09file/s, file=2023-04-17_20-21-46_scenario_1\\events.out.tfevents.1681737707\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  97%|█████████████████▍| 97/100 [00:38<00:01,  2.09file/s, file=2023-04-17_20-21-46_scenario_1\\hparams.yaml]\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  98%|▉| 98/100 [00:39<00:00,  2.09file/s, file=2023-04-17_20-21-46_scenario_1\\checkpoints\\train_epoch=4-trai\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  99%|▉| 99/100 [00:39<00:00,  2.66file/s, file=2023-04-17_20-21-46_scenario_1\\checkpoints\\train_epoch=4-trai\u001b[A\u001b[A\n",
      "\n",
      "Uploading:  99%|▉| 99/100 [00:40<00:00,  2.66file/s, file=2023-04-17_20-21-46_scenario_1\\checkpoints\\val_epoch=4-valida\u001b[A\u001b[A\n",
      "\n",
      "Uploading: 100%|█| 100/100 [00:40<00:00,  2.48file/s, file=2023-04-17_20-21-46_scenario_1\\checkpoints\\val_epoch=4-valid\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaelib.s3 import upload_folder_to_s3\n",
    "s3_folders = os.path.join(os.getenv(\"BUCKET_FOLDER\"), model_name)\n",
    "upload_folder_to_s3(f\"tensorboard/{model_name}\", s3_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f859f1e6f927a99659bbed8c715d8b2e0ee62a381a7a8caee1b1f662fc1c2ad7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
