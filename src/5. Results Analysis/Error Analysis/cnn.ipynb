{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c480828a-17af-40ec-8554-f16d6ebf945c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"cnn\"\n",
    "\n",
    "out_path = f\"pred/{model}/\"\n",
    "\n",
    "data = {\n",
    "    1 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/cnn/2023-04-25_09-07-51_scenario_1/checkpoints/val_epoch=5-validation_loss=0.2260.ckpt\",\n",
    "        \"dataset\":\"original/1.csv\",\n",
    "    },\n",
    "    3 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/cnn/2023-04-25_09-07-51_scenario_3/checkpoints/val_epoch=2-validation_loss=0.2587.ckpt\",\n",
    "        \"dataset\":\"original/3.csv\",\n",
    "    },\n",
    "    5 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/cnn/2023-04-25_09-07-51_scenario_5/checkpoints/val_epoch=2-validation_loss=0.2576.ckpt\",\n",
    "        \"dataset\":\"original/5.csv\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766c6251-3b2d-4fd4-8dd3-9aefc15b6dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset to df\n",
    "import pandas as pd\n",
    "for i in data:\n",
    "    data[i]['df'] = pd.read_csv(data[i]['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf18d51-a097-498f-9e5c-128de2a65e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyper_params = {\n",
    "    'seq_length': 256,\n",
    "    'out_feature': 2,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 16,\n",
    "    'conv_num_filters': 100,\n",
    "    'conv_kernels': (1,2,3)\n",
    "}\n",
    "\n",
    "# https://ieeexplore.ieee.org/document/8577620\n",
    "# https://ieeexplore.ieee.org/abstract/document/8589431\n",
    "# https://ieeexplore.ieee.org/document/8691381\n",
    "# https://aclanthology.org/D14-1181.pdf\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchnlp.nn as nnlp\n",
    "import lightning.pytorch as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchnlp.word_to_vector import FastText\n",
    "from torchmetrics.classification import F1Score, Accuracy, Recall, Precision\n",
    "\n",
    "class CNNClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: \"int\",\n",
    "        out_feature: \"int\",\n",
    "        fast_text_lang: 'str' = 'id',\n",
    "        fast_text_pad_sequence: 'bool' = True,\n",
    "        conv_num_filters: int = 100, \n",
    "        conv_kernels: \"tuple[int]\" = (3,4,5),\n",
    "        feed_forward_dropout: 'float' = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Config\n",
    "        self.seq_length = seq_length\n",
    "        self.fast_text_pad_sequence = fast_text_pad_sequence\n",
    "        \n",
    "        # Layer\n",
    "        self.fasttext = FastText(fast_text_lang)\n",
    "\n",
    "        self.conv_layer = nnlp.CNNEncoder(300, conv_num_filters, conv_kernels)\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Dropout(feed_forward_dropout),\n",
    "            nn.Linear(len(conv_kernels) * conv_num_filters, out_feature),\n",
    "        )\n",
    "\n",
    "        # Scorer\n",
    "        self.f1_scorer = F1Score(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.accuracy_scorer = Accuracy(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.precision_scorer = Precision(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.recall_scorer = Recall(task=\"multiclass\", num_classes=out_feature)\n",
    "\n",
    "    def _forward_fasttext(self, x: \"list[str]\"):\n",
    "        batch_text_embedding = torch.tensor([]).to(self.device)\n",
    "        for sentence in x:\n",
    "            sentence_seq = sentence.split(\" \")\n",
    "            if len(sentence_seq) > self.seq_length:\n",
    "                sentence_seq = sentence_seq[: self.seq_length]\n",
    "            if self.fast_text_pad_sequence:\n",
    "                while len(sentence_seq) < self.seq_length:\n",
    "                    sentence_seq.append(\"<PAD>\")\n",
    "            word_embedding = (\n",
    "                self.fasttext[sentence_seq].unsqueeze(0).to(self.device)\n",
    "            )\n",
    "            batch_text_embedding = torch.cat((batch_text_embedding, word_embedding))\n",
    "        return batch_text_embedding\n",
    "\n",
    "    def forward(self, x: \"list[str]\"):\n",
    "        x = self._forward_fasttext(x)\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.classification_head(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"training_loss\", loss, prog_bar=True)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"training_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"training_accuracy\", accuracy, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"validation_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"validation_accuracy\", accuracy, prog_bar=True)\n",
    "        precision = self.precision_scorer(pred, y)\n",
    "        self.log(\"validation_precision\", precision, prog_bar=True)\n",
    "        recall = self.recall_scorer(pred, y)\n",
    "        self.log(\"validation_recall\", recall, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"test_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"test_accuracy\", accuracy, prog_bar=True)\n",
    "        precision = self.precision_scorer(pred, y)\n",
    "        self.log(\"test_precision\", precision, prog_bar=True)\n",
    "        recall = self.recall_scorer(pred, y)\n",
    "        self.log(\"test_recall\", recall, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=hyper_params['learning_rate'])\n",
    "        return optimizer\n",
    "\n",
    "model_module = CNNClassifier\n",
    "def create_model():\n",
    "    return model_module(hyper_params['seq_length'], hyper_params['out_feature'], conv_num_filters=hyper_params['conv_num_filters'], conv_kernels=hyper_params['conv_kernels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef8c66b-e032-440c-890a-5db92ff147a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:01<00:00, 285.18it/s]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_10384\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:01<00:00, 281.60it/s]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_10384\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:01<00:00, 282.43it/s]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_10384\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in data:\n",
    "    print(f\"Processing scenario {i}\")\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "\n",
    "    state_dict = torch.load(data[i]['model'], map_location=\"cpu\")['state_dict']\n",
    "    model = create_model()\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    text_data = data[i]['df']['tweet'].to_list()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(text_data):\n",
    "            logits = model([f\"{text}\"])\n",
    "            predictions.append(logits)\n",
    "\n",
    "    predictions = torch.cat(predictions, 0)\n",
    "    softmax_tensor = F.softmax(predictions)\n",
    "    argmax_tensor = torch.argmax(softmax_tensor, 1)\n",
    "    np_pred = argmax_tensor.cpu().numpy()\n",
    "\n",
    "    df_pred = data[i]['df'].copy()\n",
    "    df_pred['prediction'] = np_pred\n",
    "    df_pred.to_csv(f\"{out_path}/{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e967d6a-1757-4a73-8716-c80cc649caed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
