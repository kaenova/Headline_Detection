{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60f0df0-2d54-402e-960f-c02de113da63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"indobertweet\"\n",
    "\n",
    "out_path = f\"pred/{model}/\"\n",
    "\n",
    "data = {\n",
    "    1 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/indobert/2023-04-27_04-48-48_scenario_1/checkpoints/val_epoch=2-validation_loss=0.0542.ckpt\",\n",
    "        \"dataset\":\"original/1.csv\",\n",
    "    },\n",
    "    3 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/indobert/2023-04-27_04-48-48_scenario_3/checkpoints/val_epoch=0-validation_loss=0.0793.ckpt\",\n",
    "        \"dataset\":\"original/3.csv\",\n",
    "    },\n",
    "    5 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/indobert/2023-04-27_04-48-48_scenario_5/checkpoints/val_epoch=0-validation_loss=0.0691.ckpt\",\n",
    "        \"dataset\":\"original/5.csv\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e700895-5960-45e8-82d7-2edd292edd49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset to df\n",
    "import pandas as pd\n",
    "for i in data:\n",
    "    data[i]['df'] = pd.read_csv(data[i]['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2092ff-d5d7-41d5-ab3d-4e2e3c0cdf85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\"\"\"\n",
    "https://arxiv.org/pdf/1810.04805.pdf\n",
    "\n",
    "We use a batch size of 32 and fine-tune for 3\n",
    "epochs over the data for all GLUE tasks. For each\n",
    "task, we selected the best fine-tuning learning rate\n",
    "(among 5e-5, 4e-5, 3e-5, and 2e-5)\n",
    "\"\"\"\n",
    "hyper_params = {\n",
    "    'model_name': \"indolem/indobertweet-base-uncased\",\n",
    "    'seq_length': 256,\n",
    "    'out_feature': 2,\n",
    "    'learning_rate': 2e-5,\n",
    "    'batch_size': 16\n",
    "}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning.pytorch as pl\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "from torchmetrics.classification import F1Score, Accuracy, Recall, Precision\n",
    "\n",
    "\n",
    "class BERTClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        huggingface_model_name: \"str\" = \"indolem/indobertweet-base-uncased\",\n",
    "        seq_length: \"int\" = 256,\n",
    "        out_feature: \"int\" = 2,\n",
    "        pad_sequence: \"bool\" = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.pad_sequence = pad_sequence\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(huggingface_model_name)\n",
    "        self.huggingface_model = BertForSequenceClassification.from_pretrained(\n",
    "            huggingface_model_name,\n",
    "            num_labels=out_feature,\n",
    "            problem_type=\"multi_label_classification\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.f1_scorer = F1Score(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.accuracy_scorer = Accuracy(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.precision_scorer = Precision(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.recall_scorer = Recall(task=\"multiclass\", num_classes=out_feature)\n",
    "\n",
    "    def _forward_huggingface_tokenizers(self, x: \"list[str]\"):\n",
    "        for sentence in x:\n",
    "            sentence = f\"{sentence}\"\n",
    "            sentence_seq = sentence.split(\" \")\n",
    "            if len(sentence_seq) > self.seq_length:\n",
    "                sentence_seq = sentence_seq[: self.seq_length]\n",
    "            if self.pad_sequence:\n",
    "                while len(sentence_seq) < self.seq_length:\n",
    "                    sentence_seq.append(\"[PAD]\")\n",
    "        tokens = self.tokenizer(\n",
    "            x,\n",
    "            max_length=512, # Max BERT tokens\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = tokens[\"input_ids\"].to(self.device)  # type: ignore\n",
    "        attention_mask = tokens[\"attention_mask\"].to(self.device)  # type: ignore\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    def forward(self, x: \"list[str]\") -> \"torch.Tensor\":\n",
    "        # Prepare str\n",
    "        input_ids, attention_mask = self._forward_huggingface_tokenizers(x)\n",
    "        logits = self.huggingface_model(input_ids=input_ids, attention_mask=attention_mask).logits  # type: ignore\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"training_loss\", loss)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"training_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"training_accuracy\", accuracy, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"validation_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"validation_accuracy\", accuracy, prog_bar=True)\n",
    "        precision = self.precision_scorer(pred, y)\n",
    "        self.log(\"validation_precision\", precision, prog_bar=True)\n",
    "        recall = self.recall_scorer(pred, y)\n",
    "        self.log(\"validation_recall\", recall, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"test_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"test_accuracy\", accuracy, prog_bar=True)\n",
    "        precision = self.precision_scorer(pred, y)\n",
    "        self.log(\"test_precision\", precision, prog_bar=True)\n",
    "        recall = self.recall_scorer(pred, y)\n",
    "        self.log(\"test_recall\", recall, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=hyper_params['learning_rate'])\n",
    "        return optimizer\n",
    "\n",
    "model_module = BERTClassifier\n",
    "def create_model():\n",
    "    return model_module(hyper_params['model_name'], hyper_params['seq_length'], hyper_params['out_feature'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514b4e36-3ae1-48a5-98ae-a113935fe420",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobertweet-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 381/381 [06:40<00:00,  1.05s/it]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_14436\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobertweet-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 381/381 [07:10<00:00,  1.13s/it]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_14436\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at indolem/indobertweet-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 381/381 [06:47<00:00,  1.07s/it]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_14436\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in data:\n",
    "    print(f\"Processing scenario {i}\")\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "\n",
    "    state_dict = torch.load(data[i]['model'], map_location=\"cpu\")['state_dict']\n",
    "    model = create_model()\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    text_data = data[i]['df']['tweet'].to_list()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(text_data):\n",
    "            logits = model([f\"{text}\"])\n",
    "            predictions.append(logits)\n",
    "\n",
    "    predictions = torch.cat(predictions, 0)\n",
    "    softmax_tensor = F.softmax(predictions)\n",
    "    argmax_tensor = torch.argmax(softmax_tensor, 1)\n",
    "    np_pred = argmax_tensor.cpu().numpy()\n",
    "\n",
    "    df_pred = data[i]['df'].copy()\n",
    "    df_pred['prediction'] = np_pred\n",
    "    df_pred.to_csv(f\"{out_path}/{i}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
