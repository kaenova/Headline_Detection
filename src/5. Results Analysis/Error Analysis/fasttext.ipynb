{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c4c8769-a292-495e-83a5-dccafee2120a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = \"fasttext\"\n",
    "\n",
    "out_path = f\"pred/{model}/\"\n",
    "\n",
    "data = {\n",
    "    1 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/fasttext/2023-04-25_08-31-02_scenario_1/checkpoints/val_epoch=53-validation_loss=0.2726.ckpt\",\n",
    "        \"dataset\":\"original/1.csv\",\n",
    "    },\n",
    "    3 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/fasttext/2023-04-25_08-31-02_scenario_3/checkpoints/val_epoch=50-validation_loss=0.2775.ckpt\",\n",
    "        \"dataset\":\"original/3.csv\",\n",
    "    },\n",
    "    5 : {\n",
    "        \"model\": \"../tensorboard_checkpoint/fasttext/2023-04-25_08-31-02_scenario_5/checkpoints/val_epoch=48-validation_loss=0.2798.ckpt\",\n",
    "        \"dataset\":\"original/5.csv\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7a44ff-990c-4a67-a119-dd38f84a2676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset to df\n",
    "import pandas as pd\n",
    "for i in data:\n",
    "    data[i]['df'] = pd.read_csv(data[i]['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c3126a0-3eb5-4dd0-aacf-4bb1b7ce0a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "\n",
    "# Hyperparameters\n",
    "hyper_params = {\n",
    "    'seq_length': 256,\n",
    "    'out_feature': 2,\n",
    "    'learning_rate': 8e-5,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "# https://arxiv.org/pdf/1607.01759.pdf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchnlp.word_to_vector import FastText\n",
    "import lightning.pytorch as pl\n",
    "from torchmetrics.classification import F1Score, Accuracy, Recall, Precision\n",
    "\n",
    "\n",
    "class FastTextClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: \"int\" = 256,\n",
    "        out_feature: \"int\" = 2,\n",
    "        pad_sequence: \"bool\" = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.pad_sequence = pad_sequence\n",
    "        self.fasttext = FastText(\"id\")\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(300, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_feature),\n",
    "        )\n",
    "        self.f1_scorer = F1Score(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.accuracy_scorer = Accuracy(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.precision_scorer = Precision(task=\"multiclass\", num_classes=out_feature)\n",
    "        self.recall_scorer = Recall(task=\"multiclass\", num_classes=out_feature)\n",
    "\n",
    "    def _forward_fasttext(self, x: \"list[str]\"):\n",
    "        batch_text_embedding = torch.tensor([]).to(self.device)\n",
    "        for sentence in x:\n",
    "            sentence_seq = sentence.split(\" \")\n",
    "            if len(sentence_seq) > self.seq_length:\n",
    "                sentence_seq = sentence_seq[: self.seq_length]\n",
    "            if self.pad_sequence:\n",
    "                while len(sentence_seq) < 256:\n",
    "                    sentence_seq.append(\"<pad>\")\n",
    "            word_embedding = (\n",
    "                self.fasttext[sentence_seq].mean(dim=0).unsqueeze(0).to(self.device)\n",
    "            )\n",
    "            batch_text_embedding = torch.cat((batch_text_embedding, word_embedding))\n",
    "        return batch_text_embedding\n",
    "\n",
    "    def forward(self, x: \"list[str]\") -> \"torch.Tensor\":\n",
    "        # Prepare str\n",
    "        logits: torch.Tensor = self._forward_fasttext(x)\n",
    "        logits = self.feed_forward(logits)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"training_loss\", loss)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"training_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"training_accuracy\", accuracy, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"validation_loss\", loss, prog_bar=True)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"validation_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"validation_accuracy\", accuracy, prog_bar=True)\n",
    "        precision = self.precision_scorer(pred, y)\n",
    "        self.log(\"validation_precision\", precision, prog_bar=True)\n",
    "        recall = self.recall_scorer(pred, y)\n",
    "        self.log(\"validation_recall\", recall, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        pred = self.forward(x[0])\n",
    "        loss = F.cross_entropy(pred, y)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        f1_score = self.f1_scorer(pred, y)\n",
    "        self.log(\"test_f1\", f1_score, prog_bar=True)\n",
    "        accuracy = self.accuracy_scorer(pred, y)\n",
    "        self.log(\"test_accuracy\", accuracy, prog_bar=True)\n",
    "        precision = self.precision_scorer(pred, y)\n",
    "        self.log(\"test_precision\", precision, prog_bar=True)\n",
    "        recall = self.recall_scorer(pred, y)\n",
    "        self.log(\"test_recall\", recall, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=hyper_params['learning_rate'])\n",
    "        return optimizer\n",
    "\n",
    "model_module = FastTextClassifier\n",
    "def create_model():\n",
    "    return model_module(hyper_params['seq_length'], hyper_params['out_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d53425d8-4fba-42ea-bc79-d44a7a3be51c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:00<00:00, 2048.41it/s]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_20792\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:00<00:00, 2822.27it/s]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_20792\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 381/381 [00:00<00:00, 2591.76it/s]\n",
      "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_20792\\2658260308.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_tensor = F.softmax(predictions)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in data:\n",
    "    print(f\"Processing scenario {i}\")\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "\n",
    "    state_dict = torch.load(data[i]['model'], map_location=\"cpu\")['state_dict']\n",
    "    model = create_model()\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    text_data = data[i]['df']['tweet'].to_list()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(text_data):\n",
    "            logits = model([f\"{text}\"])\n",
    "            predictions.append(logits)\n",
    "\n",
    "    predictions = torch.cat(predictions, 0)\n",
    "    softmax_tensor = F.softmax(predictions)\n",
    "    argmax_tensor = torch.argmax(softmax_tensor, 1)\n",
    "    np_pred = argmax_tensor.cpu().numpy()\n",
    "\n",
    "    df_pred = data[i]['df'].copy()\n",
    "    df_pred['prediction'] = np_pred\n",
    "    df_pred.to_csv(f\"{out_path}/{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb7969f-929f-4286-9bbf-e161a6d50445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
